{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd9415f",
   "metadata": {},
   "source": [
    "# Assignment 4 – Convolutional Variational Autoencoder (CVAE)\n",
    "\n",
    "This notebook implements a 4‑layer convolutional Variational Autoencoder with a 2‑dimensional latent space, trained on the provided synthetic 3D shapes dataset. It follows the TensorFlow CVAE tutorial structure and includes short report-style explanations for each rubric item:\n",
    "\n",
    "- Data preprocessing\n",
    "- Model architecture\n",
    "- Training procedure\n",
    "- Interpretation of the 2D latent manifold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efd8108",
   "metadata": {},
   "source": [
    "## 1. Data Generation and Visualization\n",
    "\n",
    "In this section we:\n",
    "1. Import the required libraries.\n",
    "2. Use the provided `create_enhanced_3d_shapes` function to generate the dataset.\n",
    "3. Visualize a few example images from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c479ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07adbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_3d_shapes(num_samples=8000, img_size=64):\n",
    "    \"\"\"Enhanced 3D shapes with better quality\"\"\"\n",
    "    images = []\n",
    "    \n",
    "    # Pre-defined color schemes\n",
    "    room_schemes = [\n",
    "        (np.array([0.9, 0.95, 1.0]), np.array([0.4, 0.5, 0.6])),  # Blue\n",
    "        (np.array([1.0, 0.98, 0.95]), np.array([0.6, 0.5, 0.4])), # Warm\n",
    "        (np.array([0.95, 1.0, 0.98]), np.array([0.5, 0.6, 0.4])), # Green\n",
    "    ]\n",
    "    \n",
    "    obj_colors = np.array([\n",
    "        [0.95, 0.3, 0.2],  # Red\n",
    "        [0.2, 0.95, 0.3],  # Green\n",
    "        [0.2, 0.4, 0.95],  # Blue\n",
    "        [0.95, 0.85, 0.1], # Yellow\n",
    "        [0.8, 0.2, 0.95],  # Purple\n",
    "        [0.1, 0.95, 0.95], # Cyan\n",
    "    ])\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        img = np.zeros((img_size, img_size, 3))\n",
    "        \n",
    "        # Select colors\n",
    "        wall_color, floor_color = room_schemes[np.random.randint(len(room_schemes))]\n",
    "        obj_color = obj_colors[np.random.randint(len(obj_colors))]\n",
    "        shape_type = np.random.choice(['sphere', 'cube', 'cylinder'])\n",
    "        \n",
    "        # Create room with gradient\n",
    "        floor_level = img_size * 2 // 3\n",
    "        \n",
    "        # Wall gradient\n",
    "        for y in range(floor_level):\n",
    "            factor = 0.7 + 0.3 * (y / floor_level)\n",
    "            img[y, :] = wall_color * factor\n",
    "        \n",
    "        # Floor gradient\n",
    "        for y in range(floor_level, img_size):\n",
    "            factor = 0.6 + 0.4 * ((y - floor_level) / (img_size - floor_level))\n",
    "            img[y, :] = floor_color * factor\n",
    "        \n",
    "        # Object parameters\n",
    "        obj_size = np.random.randint(20, 35)\n",
    "        pos_x = np.random.randint(30, img_size - 30)\n",
    "        pos_y = floor_level - obj_size // 2\n",
    "        \n",
    "        if shape_type == 'sphere':\n",
    "            # Enhanced sphere with lighting\n",
    "            y_arr, x_arr = np.indices((img_size, img_size))\n",
    "            dist_sq = (x_arr - pos_x)**2 + (y_arr - pos_y)**2\n",
    "            radius = obj_size // 2\n",
    "            sphere_mask = dist_sq <= radius**2\n",
    "            \n",
    "            if np.any(sphere_mask):\n",
    "                # Simple lighting\n",
    "                dist = np.sqrt(dist_sq[sphere_mask])\n",
    "                lighting = 0.4 + 0.6 * (1 - dist / radius)\n",
    "                sphere_y, sphere_x = np.where(sphere_mask)\n",
    "                img[sphere_y, sphere_x] = obj_color * lighting[:, np.newaxis]\n",
    "                \n",
    "                # Highlight\n",
    "                highlight_mask = (x_arr - pos_x + radius//3)**2 + (y_arr - pos_y + radius//3)**2 <= (radius//4)**2\n",
    "                img[highlight_mask] = np.clip(obj_color * 1.3, 0, 1)\n",
    "            \n",
    "        elif shape_type == 'cube':\n",
    "            # Enhanced cube\n",
    "            half = obj_size // 2\n",
    "            \n",
    "            # Front face\n",
    "            img[pos_y-half:pos_y+half, pos_x-half:pos_x+half] = obj_color\n",
    "            \n",
    "            # Right side\n",
    "            img[pos_y-half:pos_y+half, pos_x+half:pos_x+half+half//2] = obj_color * 0.7\n",
    "            \n",
    "            # Top side\n",
    "            img[pos_y-half-half//3:pos_y-half, pos_x-half:pos_x+half+half//2] = obj_color * 1.1\n",
    "            \n",
    "        elif shape_type == 'cylinder':\n",
    "            # Enhanced cylinder\n",
    "            radius = obj_size // 3\n",
    "            height = obj_size\n",
    "            \n",
    "            # Body\n",
    "            img[pos_y-height//2:pos_y+height//2, pos_x-radius:pos_x+radius] = obj_color * 0.8\n",
    "            \n",
    "            # Top\n",
    "            y_arr, x_arr = np.indices((img_size, img_size))\n",
    "            top_mask = (x_arr - pos_x)**2 + (y_arr - (pos_y - height//2))**2 <= radius**2\n",
    "            img[top_mask] = obj_color * 1.1\n",
    "            \n",
    "            # Bottom\n",
    "            bottom_mask = (x_arr - pos_x)**2 + (y_arr - (pos_y + height//2))**2 <= radius**2\n",
    "            img[bottom_mask] = obj_color * 0.6\n",
    "        \n",
    "        images.append(np.clip(img, 0, 1))\n",
    "    \n",
    "    return np.array(images)\n",
    "\n",
    "print('Creating enhanced 3D shapes dataset...')\n",
    "x_train = create_enhanced_3d_shapes(10000, 64).astype('float32')\n",
    "print('x_train shape:', x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85791839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show samples from x_train\n",
    "fig, axes = plt.subplots(1, 6, figsize=(18, 8))\n",
    "for i in range(6):\n",
    "    axes[i].imshow(x_train[i])\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Images from X_train', fontsize=16, y=0.95)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e0627",
   "metadata": {},
   "source": [
    "### Short Report – Data Preprocessing\n",
    "\n",
    "The dataset is generated synthetically using the provided `create_enhanced_3d_shapes` function. Each image is a **64×64 RGB frame** representing a simple 3D scene: a room with a wall and floor rendered via color gradients, and a single colored object (sphere, cube, or cylinder) placed in the room.\n",
    "\n",
    "Key preprocessing characteristics:\n",
    "\n",
    "- **On-the-fly generation**: We programmatically create 10,000 images, so there is no need for separate download or manual cleaning.\n",
    "- **Resolution and channels**: Images are created at a fixed size of 64×64 with 3 color channels (RGB), which matches the input shape expected by the convolutional encoder.\n",
    "- **Value range**: Colors are constructed from NumPy arrays in the range `[0, 1]` and explicitly clipped, so `x_train` is already normalized to `[0, 1]`. This is compatible with the final sigmoid activation and the binary cross-entropy reconstruction loss in the VAE.\n",
    "- **Variation**: Randomized wall/floor color schemes, object colors, shapes, sizes, and positions provide a reasonably diverse training set that encourages the VAE to learn meaningful latent factors related to shape type, color, and spatial location.\n",
    "\n",
    "Because the data are fully synthetic and well-controlled, no additional preprocessing steps (such as resizing, standardization, or train/validation split for noise removal) are strictly required for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c0d497",
   "metadata": {},
   "source": [
    "## 2. Model Architecture – Convolutional VAE (Latent Dim = 2)\n",
    "\n",
    "We now build a **convolutional Variational Autoencoder (VAE)** with a **2‑dimensional latent space**. The encoder uses convolutional layers to compress the 64×64×3 input into the latent representation, while the decoder mirrors this structure using transposed convolutions to reconstruct images from latent codes.\n",
    "\n",
    "First, we define a small `Sampling` layer that implements the reparameterization trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefa6b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z via reparameterization.\"\"\"\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2d2436",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2  # coding size\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(64, 64, 3))\n",
    "\n",
    "# Convolutional encoder with four main learnable layers (3 Conv2D + 1 Dense)\n",
    "x = layers.Conv2D(32, 3, strides=2, padding='same', activation='relu')(encoder_inputs)   # 32x32x32\n",
    "x = layers.Conv2D(64, 3, strides=2, padding='same', activation='relu')(x)                # 16x16x64\n",
    "x = layers.Conv2D(128, 3, strides=2, padding='same', activation='relu')(x)               # 8x8x128\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "\n",
    "z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa6706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "x = layers.Dense(8 * 8 * 128, activation='relu')(latent_inputs)\n",
    "x = layers.Reshape((8, 8, 128))(x)\n",
    "\n",
    "x = layers.Conv2DTranspose(128, 3, strides=2, padding='same', activation='relu')(x)  # 16x16x128\n",
    "x = layers.Conv2DTranspose(64, 3, strides=2, padding='same', activation='relu')(x)   # 32x32x64\n",
    "x = layers.Conv2DTranspose(32, 3, strides=2, padding='same', activation='relu')(x)   # 64x64x32\n",
    "\n",
    "decoder_outputs = layers.Conv2DTranspose(3, 3, padding='same', activation='sigmoid')(x)  # 64x64x3\n",
    "\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b5283d",
   "metadata": {},
   "source": [
    "### Short Report – Model Architecture\n",
    "\n",
    "The implemented model is a **convolutional VAE** with a **2‑dimensional latent space**, designed to capture high‑level factors of variation (shape, color, position) from 64×64×3 images.\n",
    "\n",
    "**Encoder**\n",
    "\n",
    "- Input: 64×64×3 RGB image.\n",
    "- Three convolutional layers:\n",
    "  - Conv2D(32 filters, kernel 3, stride 2, ReLU, same padding) → 32×32×32 feature maps.\n",
    "  - Conv2D(64 filters, kernel 3, stride 2, ReLU) → 16×16×64.\n",
    "  - Conv2D(128 filters, kernel 3, stride 2, ReLU) → 8×8×128.\n",
    "- Features are flattened and passed through a Dense layer with 256 units (ReLU).\n",
    "- Two separate Dense layers output the latent parameters:\n",
    "  - `z_mean` ∈ ℝ² (mean of the approximate posterior).\n",
    "  - `z_log_var` ∈ ℝ² (log-variance).\n",
    "- A custom `Sampling` layer applies the reparameterization trick to generate latent codes `z` from the Gaussian distribution defined by `(z_mean, z_log_var)`.\n",
    "\n",
    "**Decoder**\n",
    "\n",
    "- Input: a 2‑dimensional latent vector.\n",
    "- Dense layer projects the latent code to 8×8×128 and reshapes it into a feature map.\n",
    "- Three transposed convolution layers progressively upsample back to 64×64:\n",
    "  - Conv2DTranspose(128, kernel 3, stride 2, ReLU) → 16×16×128.\n",
    "  - Conv2DTranspose(64, kernel 3, stride 2, ReLU) → 32×32×64.\n",
    "  - Conv2DTranspose(32, kernel 3, stride 2, ReLU) → 64×64×32.\n",
    "- Final Conv2DTranspose with 3 filters and sigmoid activation outputs a 64×64×3 image with values in `[0,1]`.\n",
    "\n",
    "**Architectural rationale**\n",
    "\n",
    "- Convolutions leverage local spatial structure in images, making the encoder/decoder more parameter‑efficient than fully‑connected layers for this resolution.\n",
    "- A **2D latent space** enforces a compact representation that is easy to visualize as a 2D manifold.\n",
    "- The overall depth (three convolutional layers + one dense layer in the encoder, mirrored in the decoder) matches the requirement of a **four‑layer convolutional architecture** while remaining small enough to train within reasonable time and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4473d0",
   "metadata": {},
   "source": [
    "## 3. Training the VAE with a Custom `train_step`\n",
    "\n",
    "We now wrap the encoder and decoder into a custom `VAE` class, using the simplified template provided in the assignment. The model is trained with:\n",
    "\n",
    "- **Reconstruction loss**: binary cross‑entropy between input and output, summed over all pixels and channels.\n",
    "- **KL divergence**: regularizes the latent distribution towards a standard normal prior.\n",
    "\n",
    "The combination of these terms encourages the model to learn meaningful, smooth latent representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a879e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name='total_loss')\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name='reconstruction_loss')\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name='kl_loss')\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Support the (inputs, labels) style by discarding labels if present\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "\n",
    "            # Reconstruction loss: sum over H, W, C for each sample\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2, 3)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # KL divergence term\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            'loss': self.total_loss_tracker.result(),\n",
    "            'reconstruction_loss': self.reconstruction_loss_tracker.result(),\n",
    "            'kl_loss': self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88518fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4))\n",
    "\n",
    "epochs = 30  # you can lower this (e.g., 15–20) if training time is long\n",
    "batch_size = 64\n",
    "\n",
    "history = vae.fit(\n",
    "    x_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd15ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot total, reconstruction, and KL losses over epochs\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history.history['loss'], label='Total loss')\n",
    "plt.plot(history.history['reconstruction_loss'], label='Reconstruction loss')\n",
    "plt.plot(history.history['kl_loss'], label='KL loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('VAE Training Curves')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb6f716",
   "metadata": {},
   "source": [
    "### Short Report – Training Procedure\n",
    "\n",
    "The VAE is trained end‑to‑end using the custom `train_step` method. At each iteration:\n",
    "\n",
    "1. The encoder maps each input image to `(z_mean, z_log_var)` and samples a latent code `z`.\n",
    "2. The decoder reconstructs an image from `z`.\n",
    "3. The **reconstruction loss** is computed as binary cross‑entropy between the input and reconstructed images, summed over all spatial positions and color channels and averaged over the batch.\n",
    "4. The **KL divergence** term penalizes deviations of the approximate posterior `q(z|x)` from the unit Gaussian prior `p(z) = N(0, I)`.\n",
    "5. The total loss is the sum of reconstruction and KL terms; gradients of this loss are applied to all encoder and decoder parameters using the Adam optimizer.\n",
    "\n",
    "In practice, the reconstruction loss dominates early in training, with the KL term gradually increasing as the latent space becomes more structured. The training curves typically show a smooth decrease in total and reconstruction loss over epochs, indicating that the model is learning to reconstruct scenes while maintaining a regularized, continuous latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a757f657",
   "metadata": {},
   "source": [
    "## 4. Latent Space Visualization and 2D Manifold\n",
    "\n",
    "Because the latent space is 2‑dimensional, we can directly visualize how different latent codes map to images by decoding a grid of `(z₁, z₂)` values. This produces a **2D manifold** where nearby points correspond to visually similar scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01025a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_manifold(decoder, n=10, img_size=64):\n",
    "    \"\"\"Plots an n x n grid of decoded images from the 2D latent space.\"\"\"\n",
    "    grid_x = np.linspace(-3, 3, n)\n",
    "    grid_y = np.linspace(-3, 3, n)\n",
    "\n",
    "    manifold = np.zeros((n * img_size, n * img_size, 3))\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]], dtype='float32')  # shape (1, 2)\n",
    "            x_decoded = decoder.predict(z_sample, verbose=0)[0]\n",
    "            h_start = i * img_size\n",
    "            h_end = (i + 1) * img_size\n",
    "            w_start = j * img_size\n",
    "            w_end = (j + 1) * img_size\n",
    "            manifold[h_start:h_end, w_start:w_end, :] = x_decoded\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(manifold)\n",
    "    plt.axis('off')\n",
    "    plt.title('2D Latent Manifold of Generated 3D Shapes')\n",
    "    plt.show()\n",
    "\n",
    "# Generate and plot the manifold (adjust n for resolution vs. time)\n",
    "plot_latent_manifold(decoder, n=10, img_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24155b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_reconstructions(vae_model, x_data, n=6):\n",
    "    idx = np.random.choice(len(x_data), n, replace=False)\n",
    "    x_sample = x_data[idx]\n",
    "\n",
    "    z_mean, z_log_var, z = vae_model.encoder.predict(x_sample, verbose=0)\n",
    "    x_recon = vae_model.decoder.predict(z, verbose=0)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    for i in range(n):\n",
    "        # Original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(x_sample[i])\n",
    "        ax.axis('off')\n",
    "        if i == 0:\n",
    "            ax.set_title('Original')\n",
    "\n",
    "        # Reconstruction\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(x_recon[i])\n",
    "        ax.axis('off')\n",
    "        if i == 0:\n",
    "            ax.set_title('Reconstruction')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a few original vs reconstructed images\n",
    "show_reconstructions(vae, x_train, n=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edb36d8",
   "metadata": {},
   "source": [
    "### Short Report – Interpretation of the 2D Latent Manifold\n",
    "\n",
    "The 2D manifold plot decodes a regular grid of latent vectors `(z₁, z₂)` to images, showing how moving in latent space affects the generated scenes. Typical patterns observed:\n",
    "\n",
    "- **Smooth transitions**: Neighboring grid cells often correspond to objects with similar colors, shapes, or positions. This indicates that the VAE has learned a **continuous latent representation**, where small changes in `(z₁, z₂)` lead to small changes in the image.\n",
    "- **Factor mixing**: One axis of the grid may roughly correspond to changing the object shape (e.g., sphere → cube → cylinder), while the other axis may capture color variations or slight changes in the object’s horizontal position. In practice, the factors are often entangled, but clear trends usually emerge.\n",
    "- **Coverage of modes**: Different regions of the latent space generate different combinations of wall/floor colors and object appearances. If large areas of the grid appear blank or contain unrealistic artifacts, it would suggest that the latent space is not fully utilized or that training is insufficient.\n",
    "\n",
    "In addition, side‑by‑side plots of **original vs reconstructed images** show that the VAE generally captures the global structure of each scene (room layout, object location, and coarse shape) while sometimes smoothing fine details such as sharp edges or lighting highlights. This behavior is expected for VAEs, which prioritize smooth, probabilistic representations over exact pixel‑perfect reconstructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1eab7d",
   "metadata": {},
   "source": [
    "---\n",
    "**Summary:**\n",
    "\n",
    "- We generated a synthetic dataset of 64×64 RGB 3D‑like scenes.\n",
    "- We implemented a 4‑layer convolutional VAE with a 2D latent space.\n",
    "- The model was trained using a custom `train_step` with reconstruction and KL losses.\n",
    "- We visualized both the learned 2D latent manifold and example reconstructions to interpret how the model organizes different scenes in latent space.\n",
    "\n",
    "You can use the plots and short report sections from this notebook directly in your assignment write‑up."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
